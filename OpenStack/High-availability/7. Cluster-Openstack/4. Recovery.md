

## 1. Kiểm tra trạng thái trên Controller1

- Kiểm tra status MariaDB
```

MariaDB [(none)]> SHOW STATUS LIKE 'wsrep_cluster_status';
+----------------------+-------------+
| Variable_name        | Value       |
+----------------------+-------------+
| wsrep_cluster_status | non-Primary |

```

- Kiểm tra status RabbitMQ
```
# rabbitmqctl cluster_status
Cluster status of node rabbit@controller01
[{nodes,[{disc,[rabbit@controller01,rabbit@controller02,
                rabbit@controller03]}]},
 {running_nodes,[rabbit@controller01]},
 {cluster_name,<<"rabbit@controller01">>},
 {partitions,[]},
 {alarms,[{rabbit@controller01,[]}]}]

```

- Kiểm tra status Pacemaker
```
# pcs status --full


Cluster name: ha_cluster
Stack: corosync
Current DC: controller01 (1) (version 1.1.19-8.el7_6.4-c3c624ea3d) - partition WITHOUT quorum
Last updated: Tue Jul 16 14:02:49 2019
Last change: Tue Jul 16 00:37:47 2019 by root via crm_resource on controller02

3 nodes configured
3 resources configured

Online: [ controller01 (1) ]
OFFLINE: [ controller02 (2) controller03 (3) ]

Full list of resources:

 vip_public     (ocf::heartbeat:IPaddr2):       Started controller01
 VIP_Horizon    (ocf::heartbeat:IPaddr2):       Started controller01
 p_haproxy      (systemd:haproxy):      Started controller01

Node Attributes:
* Node controller01 (1):

Migration Summary:
* Node controller01 (1):

PCSD Status:
  controller03: Offline
  controller02: Offline
  controller01: Online

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

```


## 2. Khôi phục Galera Cluster

### 2.1. Update NTP

- Nếu Controller chính làm NTP đã bị down, update lại allow IP cho Controller đang ở trạng thái up 

```
sed -i 's/#allow 192.168.0.0\/16/allow 192.168.70.0\/24/g' /etc/chrony.conf
systemctl restart chronyd.service

```

### 2.2. Promote Galera Controller1

- Promote node thành Primary Compoment
```

MariaDB [(none)]>  SET GLOBAL wsrep_provider_options='pc.bootstrap=YES';

```

 - Trên các node Còn lại thực hiện khởi tạo cấu hình và khởi động dịch vụ 
```
systemctl start mariadb
systemctl enable mariadb 
```

- Log trên Controller1
```

Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631110043392 [Note] WSREP: Flow-control interval: [23, 23]
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631110043392 [Note] WSREP: Trying to continue unpaused monitor
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631101650688 [Note] WSREP: New cluster view: global state: 2698d16f-a70e-11e9-a86f-465f8439a82c:56982, view# 56: Primary, number of nodes: 2, my index: 1, protocol version 3
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631101650688 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631101650688 [Note] WSREP: REPL Protocols: 9 (4, 2)
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631101650688 [Note] WSREP: Assign initial position for certification: 56982, protocol version: 4
Jul 16 14:19:34 controller01 mysqld: 2019-07-16 14:19:34 139631152006912 [Note] WSREP: Service thread queue flushed.
Jul 16 14:19:35 controller01 xinetd[14435]: EXIT: mysqlchk status=0 pid=18881 duration=1(sec)
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139631110043392 [Note] WSREP: Member 0.0 (controller02) requested state transfer from '*any*'. Selected 1.0 (controller01)(SYNCED) as donor.
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139631110043392 [Note] WSREP: Shifting SYNCED -> DONOR/DESYNCED (TO: 56982)
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139631101650688 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139630040061696 [Note] WSREP: Running: 'wsrep_sst_rsync --role 'donor' --address '192.168.70.122:4444/rsync_sst' --socket '/var/lib/mysql/mysql.sock' --datadir '/var/lib/mysql/'     '' --gtid '2698d16f-a70e-11e9-a86f-465f8439a82c:56982' --gtid-domain-id '0''
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139631101650688 [Note] WSREP: sst_donor_thread signaled with 0
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139630040061696 [Note] WSREP: Flushing tables for SST...
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139630040061696 [Note] WSREP: Provider paused at 2698d16f-a70e-11e9-a86f-465f8439a82c:56982 (58156)
Jul 16 14:19:35 controller01 mysqld: 2019-07-16 14:19:35 139630040061696 [Note] WSREP: Tables flushed.
Jul 16 14:19:37 controller01 mysqld: 2019-07-16 14:19:37 139631118436096 [Note] WSREP: (7e515f83, 'tcp://0.0.0.0:4567') turning message relay requesting off

```

- Check health database
```
mysqlcheck -u root -p --all-databases --check

```

## 3. Khôi phục RabbitMQ Cluster

- Trên Controller1, xóa các Controller cũ khỏi cluster
```
forget_cluster_node rabbit@controller02
forget_cluster_node rabbit@controller03

```

- Trên Controller 1 Copy Elarg key sang các Controller mới
```
scp -p /var/lib/rabbitmq/.erlang.cookie controller02:/var/lib/rabbitmq/.erlang.cookie
scp -p /var/lib/rabbitmq/.erlang.cookie controller03:/var/lib/rabbitmq/.erlang.cookie
```

- Trên các Controller mới, thực hiện join cluster
```

chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie
chmod 400 /var/lib/rabbitmq/.erlang.cookie
systemctl restart rabbitmq-server
rabbitmqctl stop_app 
rabbitmqctl reset
rabbitmqctl join_cluster rabbit@controller01
rabbitmqctl start_app
```

- Kiểm tra trạng thái Cluster
```
# rabbitmqctl cluster_status
Cluster status of node rabbit@controller01
[{nodes,[{disc,[rabbit@controller01,rabbit@controller02]}]},
 {running_nodes,[rabbit@controller02,rabbit@controller01]},
 {cluster_name,<<"rabbit@controller01">>},
 {partitions,[]},
 {alarms,[{rabbit@controller02,[]},{rabbit@controller01,[]}]}]

```

## 4. Khôi phục Pacemaker

- Xóa các Controller cũ 
```
pcs cluster  node remove controller02 --force --request-timeout 10
pcs cluster  node remove controller03 --force --request-timeout 10

```

- Trên Controller1 thực hiện auth sang controller mới. Sau khi đã khởi động Service trên các node 
```
pcs cluster auth controller02 -u"hacluster" -p"Apnet2018"
pcs cluster auth controller03 -u"hacluster" -p"Apnet2018"
```

- pcs cluster node add 
```
pcs cluster node add controller02
pcs cluster node add controller03
```


- Khởi động node
```
pcs cluster enable --all
pcs cluster start --all
```
## 4. Khôi phục Keystone

- Trên Controller1 copy Fernet repository sang các Controller mới
```
scp -r /etc/keystone/credential-keys /etc/keystone/fernet-keys root@controller02:/etc/keystone/
scp -r /etc/keystone/credential-keys /etc/keystone/fernet-keys root@controller03:/etc/keystone/
ssh root@controller02 chown -R keystone:keystone /etc/keystone/credential-keys /etc/keystone/fernet-keys
ssh root@controller03 chown -R keystone:keystone /etc/keystone/credential-keys /etc/keystone/fernet-keys

```


## 5. Khôi phục Glance Image

- Trên Controller1, sao chép các iamge tại /var/lib/glance/images sang các Controller mới 
```
scp -r /var/lib/glance/images root@controller02:/var/lib/glance/images/
ssh root@controller02 chown -R glance:glance /var/lib/glance/images/

```


## 6. CEPH Recovery 

- Trên node CEPH coy file cấu hình
```
for ops_node in  192.168.50.133
do
scp /etc/ceph/ceph.conf $ops_node:/etc/ceph
ssh $ops_node chmod 644 /etc/ceph/ceph.conf
done
```